{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5182fd3b-fa90-4a6d-b146-4a5c631368eb",
   "metadata": {},
   "source": [
    "# Formalia\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/TheYuanLiao/comsocsci2025/wiki/Assignments) carefully before proceeding. The page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment.\n",
    "\n",
    "We teach about Pandas Dataframe and there is a video on how to use pandas and mistakes not to make when using it (see [Week 2](https://nbviewer.org/github/TheYuanLiao/comsocsci2025/blob/main/lectures/Week2.ipynb)/Prelude to part 3: Pandas Dataframes). We expect you to apply that knowledge in all exercises. A score of 3 means \"excellent\", where we expect you to deliver efficient dataframe operations, i.e., operations are implemented in a vectorized, efficient manner using pandas recommended practices.\n",
    "\n",
    "__If you fail to follow these simple instructions, it will negatively impact your grade!__\n",
    "\n",
    "**Due date and time**: The assignment is due on April 1st at 23:59. Hand in your Jupyter notebook file (with extension `.ipynb`) via DTU Learn _(Assignment 2)_. \n",
    "\n",
    "Remember to include in the first cell of your notebook:\n",
    "* the link to your group's Git repository \n",
    "* group members' contributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182d781-3727-4ff5-9b58-689381202a99",
   "metadata": {},
   "source": [
    "## Part 1: Properties of the real-world network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c586a",
   "metadata": {},
   "source": [
    "These exercises are taken from Week 5. Please note that 2 Degree distribution is taken out and 3 Shortest paths is numbered 2 here.\n",
    "> __Exercise: Analyzing Networks through a Random Model__ \n",
    ">\n",
    ">\n",
    ">\n",
    "> 1. _Random Network_: Let's start by building a Random Network, acting as a baseline (or [\"null model\"](https://en.wikipedia.org/wiki/Null_model)) to understand the Computational Social Scientists Network better.  \n",
    "> * First, calculate the probability (_p_) that makes the expected number of edges in our random network match the actual edge count in the Computational Social Scientists network. Refer to equation 3.2 in your Network Science textbook for guidance. After finding _p_, figure out the average degree (using the given formula). \n",
    "> * Now, write a function to generate a Random Network that mirrors the Computational Social Scientists network in terms of node count, using your calculated _p_. Generate a random network by linking nodes in every possible pair with probability _p_. **Hint**: you can use the function [``np.random.uniform``](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) to draw samples from a uniform probability distribution.   \n",
    "> * Visualize the network as you did for the Computational Social Scientists network in the exercise above (my version is below). \n",
    ">\n",
    "> * Answer the following questions __(max 200 words in total)__: \n",
    ">    - What regime does your random network fall into? Is it above or below the critical threshold?  \n",
    ">    - According to the textbook, what does the network's structure resemble in this regime?  \n",
    ">    - Based on your visualizations, identify the key differences between the actual and the random networks. Explain whether these differences are consistent with theoretical expectations.\n",
    ">\n",
    ">   \n",
    "> 2. _Shortest Paths_: Here, we will check if the Computational Social Scientists Network exhibits characteristics of a small-world network by analyzing its shortest paths.\n",
    "> * Begin by identifying the largest connected component within the Computational Social Scientists network. Recall that a connected component is a subset of nodes in which every pair of nodes is connected by a path. For a refresher on connected components, see section 2.9 of the Network Science book. Follow these steps:\n",
    ">    - Utilize [``nx.algorithms.connected_components``](https://networkx.org/documentation/stable//reference/algorithms/generated/networkx.algorithms.components.connected_components.html) to enumerate all connected components, which will be returned as a list of node subsets.\n",
    ">    - Select the largest subset of nodes identified in the previous step. Then, create a subgraph of your original network that includes only these nodes, using [``nx.Graph.subgraph``](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.subgraph.html). This subset forms your largest connected component, sometimes referred to as the [giant component](https://en.wikipedia.org/wiki/Giant_component).\n",
    "> * Calculate the average (unweighted) shortest path length within this giant component by employing [``nx.average_shortest_path_length``](https://networkx.org/documentation/networkx-1.3/reference/generated/networkx.average_shortest_path_length.html).\n",
    "> * Perform the same calculation for the giant component of the random network you constructed in the above exercise 1.\n",
    "> * Reflect on how the average shortest path lengths of the real and random networks compare and answer the following questions, supporting your answers with the theory from the book.\n",
    ">   - Why do you think I asked you to consider the giant component only?\n",
    ">   - Why do you think I asked you to consider unweighted edges?\n",
    ">   - Does the Computational Social Scientists network exhibit the small-world phenomenon?\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2 Network Analysis in Computational Social Science\n",
    "\n",
    "These exercises are taken from Week 6: __Exercise 1: Mixing Patterns and Assortativity__ and __Exercise 3: Zachary's karate club__.\n",
    "\n",
    "> __Exercise 1: Mixing Patterns and Assortativity__ \n",
    ">\n",
    "> __Part 1: Assortativity Coefficient__ \n",
    "> 1. *Calculate the Assortativity Coefficient* for the network based on the country of each node. Implement the calculation using the formula provided during the lecture, also available in [this paper](https://arxiv.org/pdf/cond-mat/0209450.pdf) (equation 2, here for directed networks). **Do not use the NetworkX implementation.**\n",
    ">\n",
    "> __Part 2: Configuration model__\n",
    "> In the following, we are going to assess the significance of the assortativity by comparing the network's assortativity coefficient against that of random networks generated through the configuration model.  \n",
    ">\n",
    "> 2. *Implement the configuration model* using the _double edge swap_ algorithm to generate random networks. Ensure each node retains its original degree but with altered connections. Create a function that does that by following these steps:\n",
    ">   - **a.** Create an exact copy of your original network.\n",
    ">   - **b.** Select two edges, $e_{1} = (u,v)$ and $e_{2} = (x,y)$, ensuring *u != y* and *v != x*.\n",
    ">   - **c.** Flip the direction of $e_{1}$ to $e_{1} = (v,u)$ 50% of the time. This ensure that your final results is not biased, in case your edges were sorted (they usually are). \n",
    ">   - **d.** Ensure that new edges $e_{1}' = (e_{1}[0],e_{2}[1])$ and $e_{2}' = (e_{2}[0],e_{1}[1])$ do not already exist in the network.\n",
    ">   - **e.** Remove edges $e_{1}$ and $e_{2}$ and add edges $e_{1}'$ and $e_{2}'$.\n",
    ">   - **f.** Repeat steps **b** to **e** until you have performed _E*10_ swaps, where E is the total number of edges.\n",
    "> 3. *Double check that your algorithm works well*, by showing that the degree of nodes in the original network and the new 'randomized' version of the network are the same.\n",
    ">\n",
    ">\n",
    "> __Part 3: Analyzing Assortativity in Random Networks__  \n",
    ">\n",
    "> 4. *Generate and analyze at least 100 random networks* using the configuration model. For each, calculate the assortativity with respect to the country and plot the distribution of these values. Compare the results with the assortativity of your original network to determine if connections within the same country are significantly higher than chance.\n",
    ">\n"
   ],
   "id": "32377c21368aea0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> __Exercise 2: Zachary's karate club__: In this exercise, we will work on Zarachy's karate club graph (refer to the Introduction of Chapter 9). The dataset is available in NetworkX, by calling the function [karate_club_graph](https://networkx.org/documentation/stable//auto_examples/graph/plot_karate_club.html)\n",
    ">\n",
    "> 1. Visualize the graph. Set the color of each node based on the club split (the information is stored as a node attribute). My version of the visualization is below.\n",
    ">\n",
    "> 2. Write a function to compute the __modularity__ of a graph partitioning (use **equation 9.12** in the book). The function should take a networkX Graph and a partitioning as inputs and return the modularity.\n",
    "> 3. Explain in your own words the concept of _modularity_.\n",
    "> 4. Compute the modularity of the Karate club split partitioning using the function you just wrote. Note: the Karate club split partitioning is avilable as a [node attribute](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.classes.function.get_node_attributes.html), called _\"club\"_.\n",
    "> 5. Create $1000$ randomized version of the Karate Club network using the _double edge swap_ algorithm you wrote in the exercise above 5. For each of them, compute the modularity of the \"club\" split and store it in a list.\n",
    "> 6. Compute the average and standard deviation of the modularity for the random network.\n",
    "> 7. Plot the distribution of the \"random\" modularity. Plot the actual modularity of the club split as a vertical line (use [axvline](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axvline.html)).\n",
    "> 8. Comment on the figure. Is the club split a good partitioning? Why do you think I asked you to perform a randomization experiment? What is the reason why we preserved the nodes degree?\n",
    "> 9.  Use [the Python Louvain-algorithm implementation](https://anaconda.org/auto/python-louvain) to find communities in this graph. Report the value of modularity found by the algorithm. Is it higher or lower than what you found above for the club split? What does this comparison reveal?\n",
    "> 10.  Compare the communities found by the Louvain algorithm with the club split partitioning by creating a matrix **_D_** with dimension (2 times _A_), where _A_ is the number of communities found by Louvain. We set entry _D_(_i_,_j_) to be the number of nodes that community _i_ has in common with group split _j_. The matrix **_D_** is what we call a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix). Use the confusion matrix to explain how well the communities you've detected correspond to the club split partitioning."
   ],
   "id": "c563f33d86340d30"
  },
  {
   "cell_type": "markdown",
   "id": "9f72dca3-246a-4056-b99c-2f14ccef7fef",
   "metadata": {},
   "source": [
    "## Part 3 - Words that characterize Computational Social Science communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba265f06",
   "metadata": {},
   "source": [
    "These exercises are taken from Week 8\n",
    "> __Exercise 1: TF-IDF and the Computational Social Science communities.__ The goal for this exercise is to find the words charachterizing each of the communities of Computational Social Scientists.\n",
    "> What you need for this exercise: \n",
    ">    * The assignment of each author to their network community, and the degree of each author (Week 6, Exercise 4). This can be stored in a dataframe or in two dictionaries, as you prefer.  \n",
    ">    * the tokenized _abstract_ dataframe (Week 7, Exercise 2)\n",
    ">\n",
    "> 1. First, check out [the wikipedia page for TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Explain in your own words the point of TF-IDF. \n",
    ">   * What does TF stand for? \n",
    ">   * What does IDF stand for?\n",
    "> 2. Now, we want to find out which words are important for each *community*, so we're going to create several ***large documents, one for each community***. Each document includes all the tokens of abstracts written by members of a given community. \n",
    ">   * Consider a community _c_\n",
    ">   * Find all the abstracts of papers written by a member of community _c_.\n",
    ">   * Create a long array that stores all the abstract tokens \n",
    ">   * Repeat for all the communities. \n",
    "> __Note:__ Here, to ensure your code is efficient, you shall exploit ``pandas`` builtin functions, such as [``groupby.apply``](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.apply.html) or [``explode``](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html).\n",
    "> 3. Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within the __top 5 communities__ (by number of authors). \n",
    ">   * Describe similarities and differences between the communities.\n",
    ">   * Why aren't the TFs not necessarily a good description of the communities?\n",
    ">   * Next, we calculate IDF for every word. \n",
    ">   * What base logarithm did you use? Is that important?\n",
    "> 4. We're ready to calculate TF-IDF. Do that for the __top 9 communities__ (by number of authors). Then for each community: \n",
    ">   * List the 10 top TF words \n",
    ">   * List the 10 top TF-IDF words\n",
    ">   * List the top 3 authors (by degree)\n",
    ">   * Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bfc68",
   "metadata": {},
   "source": [
    "\n",
    " __Exercise 2: The Wordcloud__. It's time to visualize our results!\n",
    "\n",
    "> * Install the [`WordCloud`](https://pypi.org/project/wordcloud/) module. \n",
    "> * Now, create word-cloud for each community. Feel free to make it as fancy or non-fancy as you like.\n",
    "> * Make sure that, together with the word cloud, you print the names of the top three authors in each community (see my plot above for inspiration). \n",
    "> * Comment on your results. What can you conclude on the different sub-communities in Computational Social Science? \n",
    "> * Look up online the top author in each community. In light of your search, do your results make sense?\n",
    "\n",
    " __Exercise 3: Computational Social Science__ \n",
    "\n",
    "> * Go back to Week 1, Exercise 1. Revise what you wrote on the topics in Computational Social Science. \n",
    "> * In light of your data-driven analysis, has your understanding of the field changed? How? __(max 150 words)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8fddad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
